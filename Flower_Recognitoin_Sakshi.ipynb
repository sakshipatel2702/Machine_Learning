{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO3MBF07MgODnDy+de+RFxN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sakshipatel2702/Machine_Learning/blob/main/Flower_Recognitoin_Sakshi.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "AdvkVjdhw1cz"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "from tensorflow.keras.preprocessing.image import load_img\n",
        "from tensorflow.keras.preprocessing import image"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oEZVd6pe2NHm",
        "outputId": "31baec42-7091-494f-ef43-84b233db7579"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Flower_Recognition_Sakshi_N01551583"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZ0VGXi22mK3",
        "outputId": "2958874a-1a71-4805-afa6-917cc806b165"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Flower_Recognition_Sakshi_N01551583\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_flowerdata = ImageDataGenerator(\n",
        "        rescale=1./255, \n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "flower_trainingset = train_flowerdata.flow_from_directory(\n",
        "        'flower_trainingset',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wir9VjEN5Zcx",
        "outputId": "7418c3cb-82c0-432d-c124-ef406d5c674b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3587 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_flowerdata = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "flower_trainingset = test_flowerdata.flow_from_directory(\n",
        "        'flower_trainingset',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQ_7zoWr8_p6",
        "outputId": "cba096d6-bdd6-4adc-abeb-987a15a6d3c8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3587 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_flowerdata = ImageDataGenerator(\n",
        "        rescale=1./255,\n",
        "        shear_range=0.2,\n",
        "        zoom_range=0.2,\n",
        "        horizontal_flip=True)\n",
        "flower_testset = test_flowerdata.flow_from_directory(\n",
        "        'flower_testset',\n",
        "        target_size=(64, 64),\n",
        "        batch_size=32,\n",
        "        class_mode='categorical')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BBe1jWJgUW-A",
        "outputId": "959842a5-ebe9-4d5c-af97-ddffff8b067a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 730 images belonging to 5 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Model with CNN"
      ],
      "metadata": {
        "id": "g4zmzzMNXeFO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn = tf.keras.models.Sequential()"
      ],
      "metadata": {
        "id": "74wQ5MsCXkSw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building Convolution Layer for Flower Images"
      ],
      "metadata": {
        "id": "GETzWCYgX8gc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.add(tf.keras.layers.Conv2D(filters=64 , kernel_size=3 , activation='relu' , input_shape=[64,64,3]))\n",
        "flower_cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "metadata": {
        "id": "FdHDads9YBhn"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: To make your cnn model more accurate for the flower recognition run the above code atleast twice to give 2 layers."
      ],
      "metadata": {
        "id": "4jF_RDB7aUmF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.add(tf.keras.layers.Conv2D(filters=64 , kernel_size=3 , activation='relu' , input_shape=[64,64,3]))\n",
        "flower_cnn.add(tf.keras.layers.MaxPool2D(pool_size=2,strides=2))"
      ],
      "metadata": {
        "id": "wDEbgDxPY08g"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.add(tf.keras.layers.Dropout(0.5))"
      ],
      "metadata": {
        "id": "uPtgqP4obEW1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.add(tf.keras.layers.Flatten())"
      ],
      "metadata": {
        "id": "d4G8SJTUbVW5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.add(tf.keras.layers.Dense(units=128, activation='relu'))"
      ],
      "metadata": {
        "id": "zSyn2rGrbk-v"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Note: The below line of code is for output layer and we are using units = 5 because we have 5 category that is Tulips, Rose, Dandelions, Daisy and Sunflower. And we have used activation='softmax' "
      ],
      "metadata": {
        "id": "RKm_97cYcVx2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.add(tf.keras.layers.Dense(units=5 , activation='softmax'))"
      ],
      "metadata": {
        "id": "zO0f2Pwnb-7T"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.compile(optimizer = 'rmsprop' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "eylL9STRcob1"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flower_cnn.fit(x = flower_trainingset , validation_data = flower_testset , epochs = 30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FE8p1BFYdvYq",
        "outputId": "d69cd3a5-6060-468a-ce7b-88e67e473935"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "113/113 [==============================] - 1359s 12s/step - loss: 1.3587 - accuracy: 0.4120 - val_loss: 1.1694 - val_accuracy: 0.4918\n",
            "Epoch 2/30\n",
            "113/113 [==============================] - 63s 557ms/step - loss: 1.1144 - accuracy: 0.5486 - val_loss: 1.1610 - val_accuracy: 0.5370\n",
            "Epoch 3/30\n",
            "113/113 [==============================] - 66s 587ms/step - loss: 1.0160 - accuracy: 0.6019 - val_loss: 1.1066 - val_accuracy: 0.5658\n",
            "Epoch 4/30\n",
            "113/113 [==============================] - 64s 564ms/step - loss: 0.9356 - accuracy: 0.6404 - val_loss: 1.0396 - val_accuracy: 0.5836\n",
            "Epoch 5/30\n",
            "113/113 [==============================] - 66s 578ms/step - loss: 0.8664 - accuracy: 0.6694 - val_loss: 1.0996 - val_accuracy: 0.5945\n",
            "Epoch 6/30\n",
            "113/113 [==============================] - 67s 596ms/step - loss: 0.8337 - accuracy: 0.6800 - val_loss: 1.0357 - val_accuracy: 0.6137\n",
            "Epoch 7/30\n",
            "113/113 [==============================] - 66s 582ms/step - loss: 0.7948 - accuracy: 0.6970 - val_loss: 0.9273 - val_accuracy: 0.6548\n",
            "Epoch 8/30\n",
            "113/113 [==============================] - 66s 583ms/step - loss: 0.7487 - accuracy: 0.7154 - val_loss: 0.8443 - val_accuracy: 0.6918\n",
            "Epoch 9/30\n",
            "113/113 [==============================] - 65s 577ms/step - loss: 0.7244 - accuracy: 0.7287 - val_loss: 0.8302 - val_accuracy: 0.6959\n",
            "Epoch 10/30\n",
            "113/113 [==============================] - 66s 585ms/step - loss: 0.6880 - accuracy: 0.7318 - val_loss: 0.8570 - val_accuracy: 0.7055\n",
            "Epoch 11/30\n",
            "113/113 [==============================] - 68s 603ms/step - loss: 0.6961 - accuracy: 0.7310 - val_loss: 0.8047 - val_accuracy: 0.6863\n",
            "Epoch 12/30\n",
            "113/113 [==============================] - 61s 542ms/step - loss: 0.6355 - accuracy: 0.7597 - val_loss: 0.8872 - val_accuracy: 0.6740\n",
            "Epoch 13/30\n",
            "113/113 [==============================] - 64s 566ms/step - loss: 0.6197 - accuracy: 0.7650 - val_loss: 0.9111 - val_accuracy: 0.6918\n",
            "Epoch 14/30\n",
            "113/113 [==============================] - 66s 583ms/step - loss: 0.5996 - accuracy: 0.7823 - val_loss: 0.9361 - val_accuracy: 0.6493\n",
            "Epoch 15/30\n",
            "113/113 [==============================] - 66s 583ms/step - loss: 0.5864 - accuracy: 0.7753 - val_loss: 0.8855 - val_accuracy: 0.6767\n",
            "Epoch 16/30\n",
            "113/113 [==============================] - 62s 552ms/step - loss: 0.5584 - accuracy: 0.7920 - val_loss: 0.8109 - val_accuracy: 0.7014\n",
            "Epoch 17/30\n",
            "113/113 [==============================] - 64s 562ms/step - loss: 0.5422 - accuracy: 0.7948 - val_loss: 0.9694 - val_accuracy: 0.6685\n",
            "Epoch 18/30\n",
            "113/113 [==============================] - 68s 602ms/step - loss: 0.5200 - accuracy: 0.8113 - val_loss: 0.8283 - val_accuracy: 0.7219\n",
            "Epoch 19/30\n",
            "113/113 [==============================] - 62s 544ms/step - loss: 0.4963 - accuracy: 0.8157 - val_loss: 0.7852 - val_accuracy: 0.7315\n",
            "Epoch 20/30\n",
            "113/113 [==============================] - 66s 578ms/step - loss: 0.4862 - accuracy: 0.8221 - val_loss: 0.9006 - val_accuracy: 0.7082\n",
            "Epoch 21/30\n",
            "113/113 [==============================] - 61s 543ms/step - loss: 0.4680 - accuracy: 0.8196 - val_loss: 0.9300 - val_accuracy: 0.7041\n",
            "Epoch 22/30\n",
            "113/113 [==============================] - 68s 602ms/step - loss: 0.4560 - accuracy: 0.8350 - val_loss: 0.8994 - val_accuracy: 0.7164\n",
            "Epoch 23/30\n",
            "113/113 [==============================] - 63s 561ms/step - loss: 0.4388 - accuracy: 0.8375 - val_loss: 0.9236 - val_accuracy: 0.7219\n",
            "Epoch 24/30\n",
            "113/113 [==============================] - 61s 536ms/step - loss: 0.4259 - accuracy: 0.8481 - val_loss: 0.9271 - val_accuracy: 0.6932\n",
            "Epoch 25/30\n",
            "113/113 [==============================] - 70s 616ms/step - loss: 0.3967 - accuracy: 0.8534 - val_loss: 0.9635 - val_accuracy: 0.7041\n",
            "Epoch 26/30\n",
            "113/113 [==============================] - 63s 555ms/step - loss: 0.3972 - accuracy: 0.8536 - val_loss: 1.0586 - val_accuracy: 0.6973\n",
            "Epoch 27/30\n",
            "113/113 [==============================] - 65s 574ms/step - loss: 0.3690 - accuracy: 0.8662 - val_loss: 1.0786 - val_accuracy: 0.6945\n",
            "Epoch 28/30\n",
            "113/113 [==============================] - 63s 556ms/step - loss: 0.3507 - accuracy: 0.8821 - val_loss: 0.9356 - val_accuracy: 0.7164\n",
            "Epoch 29/30\n",
            "113/113 [==============================] - 66s 580ms/step - loss: 0.3586 - accuracy: 0.8670 - val_loss: 0.9207 - val_accuracy: 0.7137\n",
            "Epoch 30/30\n",
            "113/113 [==============================] - 65s 576ms/step - loss: 0.3497 - accuracy: 0.8782 - val_loss: 1.0075 - val_accuracy: 0.7192\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f1519c32d30>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_flowerimg = load_img('/content/drive/MyDrive/Flower_Recognition_Sakshi_N01551583/flower_prediction/image24.jpg',target_size=(64,64))\n",
        "test_flowerimg = image.img_to_array(test_flowerimg)\n",
        "test_flowerimg = np.expand_dims(test_flowerimg,axis=0)\n",
        "flower = flower_cnn.predict(test_flowerimg)\n",
        "flower_trainingset.class_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AO1vj0Chr7f2",
        "outputId": "ab3b9908-2ddf-4ee9-9ac3-a6ce2a0c2684"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 34ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'daisy': 0, 'dandelion': 1, 'rose': 2, 'sunflower': 3, 'tulip': 4}"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(flower)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmVapmevveOO",
        "outputId": "7a6f7f9f-3a48-4fbf-d3a3-f9844675b74b"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 0. 0. 1. 0.]]\n"
          ]
        }
      ]
    }
  ]
}